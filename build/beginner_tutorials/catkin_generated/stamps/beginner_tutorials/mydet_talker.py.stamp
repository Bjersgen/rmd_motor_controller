#!/usr/bin/env python
# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license
import rospy
from std_msgs.msg import String

import argparse
import os
import sys
from pathlib import Path

import cv2
from numpy import random
import numpy as np
import torch
import torch.backends.cudnn as cudnn
import pyrealsense2 as rs	#å¯¼å…¥realsenseçš„sdkæ¨¡å—
import datetime

# è®¾ç½®è·¯å¾„
FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))

from models.experimental import attempt_load
from utils.general import check_img_size, check_requirements,  \
    non_max_suppression, print_args, scale_coords, set_logging
from utils.plots import Annotator, colors
from utils.torch_utils import select_device, time_sync
from utils.datasets import letterbox

# æ·±åº¦ä¿¡æ¯æ£€æµ‹ä¸åˆ°ä¼šæŠ¥è­¦ï¼Œå¿½ç•¥
import warnings
warnings.filterwarnings('ignore')

@torch.no_grad()
def run(weights=ROOT / 'yolov5s.pt',  # model.pt path
        imgsz=640,                    # inference size, 640Ã—480
        conf_thres=0.4,               # confidence threshold
        iou_thres=0.5,                # NMS IOU threshold
        max_det=1000,                 # maximum detections per image
        device='',                    # cuda device
        classes=None,                 # filter by class: --class 0, or --class 0 2 3
        agnostic_nms=False,           # class-agnostic NMS
        augment=False,                # augmented inference
        line_thickness=3,             # bounding box thickness
        ):
    
    # æ¶ˆæ¯å‘å¸ƒåˆå§‹åŒ–
    pub = rospy.Publisher('chatter', String, queue_size=10)
    rospy.init_node('talker', anonymous=True)
    # rate = rospy.Rate(0.2) # 0.2hz

    # åˆå§‹åŒ–
    set_logging()      # æ—¥å¿—åˆå§‹åŒ–
    device = select_device(device)
    t_soundsum = 0     # ç”¨äºè®°å½•æ¶ˆæ¯å‘å¸ƒé—´éš”æ—¶é—´
    t_set = 5          # é—´éš”æ—¶é—´è®¾ç½®
    dis_thres = 0.6    # è¾“å‡ºç‰©ä½“çš„æœ€è¿œè·ç¦»
    flag = 0           # è®°å½•é¦–æ¬¡æ¶ˆæ¯å‘å¸ƒ
    publish_strs = []  # è®°å½•æ‰€æœ‰å‘å¸ƒçš„æ¶ˆæ¯
    
    # Load model
    w = str(weights[0] if isinstance(weights, list) else weights)
    stride, names = 64, [f'class{i}' for i in range(1000)]  # assign defaults
    
    model = torch.jit.load(w) if 'torchscript' in w else attempt_load(weights, map_location=device)
    stride = int(model.stride.max())  # model stride
    names = model.module.names if hasattr(model, 'module') else model.names  # get class names
        
    imgsz = check_img_size(imgsz, s=stride)  # check image size
    
    # å®ä¾‹åŒ–realsenseæ¨¡å—
    pipeline = rs.pipeline()
    # åˆ›å»ºconfigå¯¹è±¡
    config = rs.config()
    # å£°æ˜RGBå’Œæ·±åº¦è§†é¢‘æµ
    config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)

    # å¯åŠ¨æ•°æ®æµ
    pipeline.start(config)
    align_to_color = rs.align(rs.stream.color)  # å¯¹é½rgbå’Œæ·±åº¦å›¾
    
    # å¾ªç¯æ£€æµ‹ï¼Œis_shutdown()æ£€æŸ¥ç¨‹åºæ˜¯å¦åº”è¯¥é€€å‡º
    while not rospy.is_shutdown():
        t1 = time_sync()
        obj_list = []  # ç”¨äºä¿å­˜æ¯ä¸€å¸§ä¸­æ»¡è¶³æ¡ä»¶çš„æ£€æµ‹ä¿¡æ¯ä»¥ä¾›é€‰æ‹©
        
        # ç­‰å¾…æœ€æ–°çš„å½±åƒï¼šæ·±åº¦å’Œé¢œè‰²åˆæˆçš„è¿è´¯çš„å¸§
        frames = pipeline.wait_for_frames()
        frames = align_to_color.process(frames)  # å¯¹é½
        
        # å°†åˆæˆå¸§åˆ†å¼€
        depth_frame = frames.get_depth_frame()
        color_frame = frames.get_color_frame()
        
        # è½¬æ¢æˆnumpy
        color_image = np.asanyarray(color_frame.get_data())

        # å¯¹RGBçš„imgè¿›è¡Œå¤„ç†ï¼Œé€å…¥é¢„æµ‹æ¨¡å‹ï¼›pathï¼šå›¾ç‰‡/è§†é¢‘è·¯å¾„ï¼Œimg:Padded resizeåçš„å›¾ç‰‡ï¼Œim0s:åŸå›¾
        imgs = [None]
        imgs[0] = color_image
        im0s = imgs.copy()
        img = [letterbox(x, new_shape=imgsz)[0] for x in im0s]
        img = np.stack(img, 0)
        img = img[:, :, :, ::-1].transpose(0, 3, 1, 2)     # BGR to RGB
        img = np.ascontiguousarray(img, dtype=np.float32)  # uint8 to float32
        
        img = torch.from_numpy(img).to(device)
        img = img / 255.0    # 0 - 255 to 0.0 - 1.0
        if len(img.shape) == 3:
            img = img[None]  # ç»´åº¦æ‰©å±•
     
        # é¢„æµ‹
        pred = model(img, augment=augment)[0]

        # NMS
        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)
        
        # æ‰“å°åŠä¿å­˜æ£€æµ‹ä¿¡æ¯
        for i, det in enumerate(pred):
            im0 = im0s[i].copy()

            annotator = Annotator(im0, line_width=line_thickness, example=str(names))  # ç”¨äºæ·»åŠ æ³¨é‡Š
            if len(det):
                # Rescale boxes from img_size to im0 size
                # å‚æ•°ï¼šresizeåå›¾ç‰‡å¤§å°ï¼Œè¾¹æ¡†å¤§å°ï¼ŒåŸå›¾å¤§å°
                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()

                # ç”»é¢„æµ‹æ¡†ï¼Œåæ ‡(0, 0)åœ¨å·¦ä¸Šè§’
                for *xyxy, conf, cls in reversed(det):
                                    
                    # è·å–è·ç¦»
                    distance_list = []  
                    mid_pos = [int((int(xyxy[0]) + int(xyxy[2])) / 2), int((int(xyxy[1]) + int(xyxy[3])) / 2)]  # é¢„æµ‹æ¡†çš„ä¸­å¿ƒç‚¹ä½ç½®
                    min_val = min(abs(int(xyxy[2]) - int(xyxy[0])), abs(int(xyxy[3]) - int(xyxy[1])))           # é¢„æµ‹æ¡†è¾ƒå°è¾¹çš„é•¿åº¦
                    # å¯¹40+1ä¸ªéšæœºç‚¹è¿›è¡Œæ·±åº¦å€¼è·å–
                    randnum = 40
                    dist = depth_frame.get_distance(int(mid_pos[0]), int(mid_pos[1])) # ä¿è¯è·å–äº†ä¸­å¿ƒç‚¹çš„æ·±åº¦å€¼
                    if dist:
                        distance_list.append(dist)  # æ·»åŠ åˆ°åˆ—è¡¨
                    for _ in range(randnum):
                        bias = random.randint(-min_val // 5, min_val // 5)  # ç”Ÿæˆéšæœºåå·®ï¼ŒèŒƒå›´ä¸ºÂ±é¢„æµ‹æ¡†è¾ƒå°è¾¹é•¿åº¦çš„1/5
                        dist = depth_frame.get_distance(int(mid_pos[0] + bias), int(mid_pos[1] + bias)) # è·å–è·ä¸­å¿ƒç‚¹åå·®å¤„çš„æ·±åº¦å€¼
                        if dist:
                            distance_list.append(dist)
                    # å¯¹æ·±åº¦ä¿¡æ¯è¿›è¡Œæ’åºå–å‡ºä¸­é—´å€¼
                    distance_list = np.array(distance_list)
                    distance_list = np.sort(distance_list)[randnum // 2 - randnum // 4:randnum // 2 + randnum // 4]
                    # å–å¹³å‡å€¼ä¸ºç›®æ ‡æ·±åº¦
                    distance = '%.2f%s' % (np.mean(distance_list), 'm')
                    
                    # æ˜¾ç¤ºæ ‡ç­¾ã€ç½®ä¿¡å€¼å’Œæ·±åº¦å€¼
                    c = int(cls)  # é¢„æµ‹ç±»åˆ«
                    label = f'{names[c]} {conf:.2f} {distance}'
                    # æŠŠæ³¨é‡Šç”»åˆ°è¾¹æ¡†ä¸Š
                    annotator.box_label(xyxy, label, color=colors(c, True))
                    
                    # ç­›é€‰æ»¡è¶³é¿è®©æ¡ä»¶çš„ç‰©ä½“
                    if 160 < mid_pos[1] < 480 and np.mean(distance_list) < dis_thres:
                        obj_list.append([names[c], round(np.mean(distance_list), 3)])
                    
            # æ˜¾ç¤ºåŒ…å«æ£€æµ‹ç»“æœçš„å½±åƒ
            im0 = annotator.result()     
            cv2.imshow('detection', im0)
            cv2.waitKey(1)
        
        t_soundsum += time_sync() - t1
        # æ¯éš”ä¸€å®šæ—¶é—´è¾“å‡ºå‰æ–¹æœ€è¿‘çš„ç‰©ä½“ï¼Œé¦–æ¬¡æ¶ˆæ¯å‘å¸ƒæ— éœ€æ»¡è¶³æ—¶é—´è¦æ±‚
        if len(obj_list) != 0 and (t_soundsum > t_set or flag == 0):
            flag = 1
            obj_list = sorted(obj_list, key=lambda x:x[1])
            obj_choose = obj_list[0]
            t_soundsum = 0
            # publish_str = f"{obj_choose[0]} is detected, and the distance is {obj_choose[1]}m"
            publish_str = f"{obj_choose[0]}"
            rospy.loginfo(publish_str)
            pub.publish(publish_str)
            # rate.sleep()
            
            publish_strs.append(publish_str)

    # å°†é€šä¿¡ç»“æœå†™å…¥æ–‡ä»¶
    with open('runs/conversation/results.txt', 'a+') as f:
        current_time = datetime.datetime.now()
        f.write(str(current_time) + '\n')
        f.write('\n'.join(publish_strs))
        f.write('\n\n')
    print("Results saved to runs/conversation/results.txt")


def parse_opt():
    parser = argparse.ArgumentParser()
    # æ¨¡å‹è·¯å¾„
    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path(s)')
    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')
    parser.add_argument('--conf-thres', type=float, default=0.4, help='confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.5, help='NMS IoU threshold')
    
    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    
    # ç”»çº¿è¾¹æ¡†åšåº¦
    parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)')
    opt = parser.parse_args()
    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand
    print_args(FILE.stem, opt)
    return opt


def main(opt):
    # check_requirementså¯¹pythonç‰ˆæœ¬å’Œrequirements.txtå®‰è£…çš„åŒ…è¿›è¡Œç‰ˆæœ¬æ£€æµ‹
    # run()å‡½æ•°åˆ™æ˜¯æ‰§è¡Œç›®æ ‡æ£€æµ‹çš„ä¸»å‡½æ•°
    check_requirements(exclude=('tensorboard', 'thop'))
    run(**vars(opt))


if __name__ == "__main__":
    opt = parse_opt()
    try:
        main(opt)
    except rospy.ROSInterruptException:
        pass

